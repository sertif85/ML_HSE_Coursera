{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming Assignment: Метрики качества классификации\n",
    "\n",
    "Вы научитесь:\n",
    "\n",
    "вычислять различные меры качества классификации: долю правильных ответов, точность, полноту, AUC-ROC и т.д.\n",
    "сравнивать алгоритмы классификации при наличии ограничений на точность или полноту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPFN, FPTN : 102 98\n",
      "Answer on question 1: 43 34 59 64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from __future__ import division\n",
    "\n",
    "# Загрузите файл classification.csv. \n",
    "# В нем записаны истинные классы объектов выборки (колонка true) и ответы некоторого классификатора (колонка pred).\n",
    "\n",
    "data = pd.read_csv(\"classification.csv\")\n",
    "\n",
    "# Заполните таблицу ошибок классификации:\n",
    "# Actual Positive\tActual Negative\n",
    "# Predicted Positive\tTP\tFP\n",
    "# Predicted Negative\tFN\tTN\n",
    "# Для этого подсчитайте величины TP, FP, FN и TN согласно их определениям. \n",
    "# Например, FP — это количество объектов, имеющих класс 0, но отнесенных алгоритмом к классу 1. \n",
    "# Ответ в данном вопросе — четыре числа через пробел.\n",
    "\n",
    "TPFN = data['true'].value_counts()[1]\n",
    "FPTN = data['true'].value_counts()[0]\n",
    "\n",
    "print('TPFN, FPTN : %d %d' % (TPFN, FPTN)) \n",
    "TP, FP, FN, TN = 0, 0, 0, 0\n",
    "for i in xrange(data.shape[0]):\n",
    "    if data['true'][i] == data['pred'][i]:\n",
    "        if data['true'][i] == 1:\n",
    "            TP += 1\n",
    "        if data['true'][i] == 0:\n",
    "            TN += 1\n",
    "    else:\n",
    "        if data['true'][i] == 1:\n",
    "            FN += 1\n",
    "        if data['true'][i] == 0:\n",
    "            FP += 1\n",
    "\n",
    "print('Answer on question 1: %d %d %d %d' % (TP, FP, FN, TN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer on question 2: 0.54 0.56 0.42 0.48\n"
     ]
    }
   ],
   "source": [
    "# Посчитайте основные метрики качества классификатора:\n",
    "# Accuracy (доля верно угаданных) — sklearn.metrics.accuracy_score\n",
    "# Precision (точность) — sklearn.metrics.precision_score\n",
    "# Recall (полнота) — sklearn.metrics.recall_score\n",
    "# F-мера — sklearn.metrics.f1_score\n",
    "# В качестве ответа укажите эти четыре числа через пробел.\n",
    "\n",
    "accuracy = metrics.accuracy_score(data['true'], data['pred'])\n",
    "precision = metrics.precision_score(data['true'], data['pred'])\n",
    "recall = metrics.recall_score(data['true'], data['pred'])\n",
    "f1 = metrics.f1_score(data['true'], data['pred'])\n",
    "\n",
    "print('Answer on question 2: %.2f %.2f %.2f %.2f' % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score_logreg = 0.72\n",
      "roc_auc_score_svm = 0.71\n",
      "roc_auc_score_knn = 0.64\n",
      "roc_auc_score_tree = 0.69\n",
      "max score value = 0.72\n",
      "Answer on question 3: score_logreg\n"
     ]
    }
   ],
   "source": [
    "# Имеется четыре обученных классификатора. В файле scores.csv записаны истинные классы и значения степени принадлежности \n",
    "# положительному классу для каждого классификатора на некоторой выборке:\n",
    "# для логистической регрессии — вероятность положительного класса (колонка score_logreg),\n",
    "# для SVM — отступ от разделяющей поверхности (колонка score_svm),\n",
    "# для метрического алгоритма — взвешенная сумма классов соседей (колонка score_knn),\n",
    "# для решающего дерева — доля положительных объектов в листе (колонка score_tree).\n",
    "# Загрузите этот файл.\n",
    "\n",
    "data = pd.read_csv('scores.csv')\n",
    "\n",
    "# Посчитайте площадь под ROC-кривой для каждого классификатора. \n",
    "# Какой классификатор имеет наибольшее значение метрики AUC-ROC (укажите название столбца)? \n",
    "# Воспользуйтесь функцией sklearn.metrics.roc_auc_score.\n",
    "\n",
    "roc_auc_score_logreg = metrics.roc_auc_score(data['true'], data['score_logreg'])\n",
    "roc_auc_score_svm = metrics.roc_auc_score(data['true'], data['score_svm'])\n",
    "roc_auc_score_knn = metrics.roc_auc_score(data['true'], data['score_knn'])\n",
    "roc_auc_score_tree = metrics.roc_auc_score(data['true'], data['score_tree'])\n",
    "print('roc_auc_score_logreg = %.2f' % roc_auc_score_logreg)\n",
    "print('roc_auc_score_svm = %.2f' % roc_auc_score_svm)\n",
    "print('roc_auc_score_knn = %.2f' % roc_auc_score_knn)\n",
    "print('roc_auc_score_tree = %.2f' % roc_auc_score_tree)\n",
    "\n",
    "maximum = np.max([roc_auc_score_logreg, roc_auc_score_svm, roc_auc_score_knn, roc_auc_score_tree])\n",
    "print('max score value = %.2f' % maximum)\n",
    "\n",
    "# Общее решение\n",
    "roc_auc_scores = {}\n",
    "for clf in data.columns[1:]:\n",
    "    roc_auc_scores[clf] = metrics.roc_auc_score(data['true'], data[clf])\n",
    "        \n",
    "print('Answer on question 3: %s' % pd.Series(roc_auc_scores).sort_values(ascending=False).head(1).index[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer on question 4: score_tree\n"
     ]
    }
   ],
   "source": [
    "# Какой классификатор достигает наибольшей точности (Precision) при полноте (Recall) не менее 70% ?\n",
    "\n",
    "# Найдите максимальной значение точности среди тех записей, для которых полнота не меньше, чем 0.7.\n",
    "pr_scores = {}\n",
    "for clf in data.columns[1:]:\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(data['true'], data[clf])\n",
    "    pr_scores[clf] = precision[recall >= 0.7].max()\n",
    "    \n",
    "print('Answer on question 4: %s' % pd.Series(pr_scores).sort_values(ascending=False).head(1).index[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
