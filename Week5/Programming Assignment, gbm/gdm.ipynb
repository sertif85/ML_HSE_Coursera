{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming Assignment: Градиентный бустинг над решающими деревьями\n",
    "    \n",
    "Данное задание основано на материалах лекций по композициям алгоритмов.\n",
    "\n",
    "Вы научитесь:\n",
    "\n",
    "работать с градиентным бустингом и подбирать его гиперпараметры\n",
    "сравнивать разные способы построения композиций\n",
    "понимать, в каком случае лучше использовать случайный лес, а в каком — градиентный бустинг\n",
    "использовать метрику log-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal log-loss value and its index with learning_rate 0.2 = { 0.53 36 }\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Загрузите выборку из файла gbm-data.csv с помощью pandas и преобразуйте ее в массив numpy \n",
    "# В первой колонке файла с данными записано, была или нет реакция. \n",
    "# Все остальные колонки (d1 - d1776) содержат различные характеристики молекулы, такие как размер, форма и т.д. \n",
    "# Разбейте выборку на обучающую и тестовую, используя функцию train_test_split с параметрами test_size = 0.8 и random_state = 241.\n",
    "df = pd.read_csv('gbm-data.csv')\n",
    "X = df.loc[:, 'D1':].values\n",
    "y = df['Activity'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=241)\n",
    "\n",
    "# 2. Обучите GradientBoostingClassifier с параметрами n_estimators=250, verbose=True, random_state=241 и \n",
    "# для каждого значения learning_rate из списка [1, 0.5, 0.3, 0.2, 0.1] проделайте следующее:\n",
    "# - Используйте метод staged_decision_function для предсказания качества на обучающей и тестовой выборке на каждой итерации.\n",
    "# - Преобразуйте полученное предсказание с помощью сигмоидной функции по формуле 1 / (1 + e^{−y_pred}), где y_pred — предсказанное значение.\n",
    "# - Вычислите и постройте график значений log-loss (которую можно посчитать с помощью функции ) на обучающей и тестовой выборках, а также найдите минимальное значение метрики и номер итерации, на которой оно достигается.\n",
    "\n",
    "def sigmoid(y_pred):\n",
    "    return 1.0 / (1.0 + np.exp(-y_pred))\n",
    "\n",
    "\n",
    "def log_loss_results(X, y, model):\n",
    "    results = []\n",
    "    for pred in model.staged_decision_function(X):\n",
    "        results.append(log_loss(y, [sigmoid(y_pred) for y_pred in pred]))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_log_loss(learning_rate, log_loss_train, log_loss_test):\n",
    "    plt.figure()\n",
    "    plt.plot(log_loss_test, 'r', linewidth=2)\n",
    "    plt.plot(log_loss_train, 'g', linewidth=2)\n",
    "    plt.legend(['test', 'train'])\n",
    "    plt.savefig('plots/rate_' + str(learning_rate) + '.png')\n",
    "    \n",
    "    min_log_loss_value = min(log_loss_test)\n",
    "    min_log_los_index = log_loss_test.index(min_log_loss_value)\n",
    "    \n",
    "    return min_log_loss_value, min_log_los_index\n",
    "\n",
    "def model_test(learning_rate):\n",
    "    grd = GradientBoostingClassifier(learning_rate=learning_rate, n_estimators=250, verbose=False, random_state=241)\n",
    "    grd.fit(X_train, y_train)\n",
    "\n",
    "    log_loss_train = log_loss_results(X_train, y_train, grd)\n",
    "    log_loss_test = log_loss_results(X_test, y_test, grd)\n",
    "\n",
    "    return plot_log_loss(learning_rate, log_loss_train, log_loss_test)\n",
    "\n",
    "min_log_loss_results = {}\n",
    "for learning_rate in [1, 0.5, 0.3, 0.2, 0.1]:\n",
    "    min_log_loss_results[learning_rate] = model_test(learning_rate)\n",
    "\n",
    "# 3. Как можно охарактеризовать график качества на тестовой выборке, начиная с некоторой итерации: \n",
    "# переобучение (overfitting) или недообучение (underfitting)? В ответе укажите одно из слов overfitting либо underfitting.\n",
    "\n",
    "# 4. Приведите минимальное значение log-loss на тестовой выборке и номер итерации, на котором оно достигается, при learning_rate = 0.2.\n",
    "print(\"Minimal log-loss value and its index with learning_rate 0.2 = { %.2f %d }\" % (min_log_loss_results.get(0.2)[0], min_log_loss_results.get(0.2)[1]))\n",
    "\n",
    "# 5. На этих же данных обучите RandomForestClassifier с количеством деревьев, равным количеству итераций, \n",
    "# на котором достигается наилучшее качество у градиентного бустинга из предыдущего пункта, \n",
    "# c random_state=241 и остальными параметрами по умолчанию. \n",
    "# Какое значение log-loss на тесте получается у этого случайного леса? \n",
    "# (Не забывайте, что предсказания нужно получать с помощью функции predict_proba. \n",
    "# В данном случае брать сигмоиду от оценки вероятности класса не нужно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 3 is 0.54\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=36, random_state=241)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "test_log_loss = log_loss(y_test, y_pred)\n",
    "print(\"Answer 3 is %.2f\" % test_log_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
